{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Extract Transform and Load (ETL) procedure\n",
    "\n",
    "This notebook will walk you through the process that loads the data from a FITS cube into the Parquet tables on a Hive enables HDFS cluster. \n",
    "\n",
    "## Part 2 Load the data into the Parquet table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Spark Context\n",
    "\n",
    "For these processes, we need to change the spark context parameters because we're using this notebook to dynamically insert data into Hive partitioned tables; this means we need to set the hive.exec.dynamic parameters. See the context configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([('spark.executor.memory', '8g'),\\\n",
    "                           ('spark.driver.memory', '6g'),\\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.sql.shuffle.partitions', 144), \\\n",
    "                           ('spark.default.parallelism', 144),\\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.executor.instances', 40), \\\n",
    "                           ('spark.local.dir','/mnt/FITs/Spark/tmp'),\\\n",
    "                           ('spark.jars.packages', 'com.github.astrolabsoftware:spark-fits_2.11:0.9.0'),\\\n",
    "                           ('spark.executor.memoryOverhead', '6g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.rpc.message.maxSize', '512'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '1g'),\\\n",
    "                           ('spark.driver.allowMultipleContexts', True), \\\n",
    "                           ('spark.hadoop.hive.exec.dynamic.partition', True), \\\n",
    "                           ('spark.hadoop.hive.exec.dynamic.partition.mode','nonstrict'), \\\n",
    "                           ('spark.hadoop.hive.exec.max.dynamic.partitions', 100000), \\\n",
    "                           ('spark.hadoop.hive.exec.max.dynamic.partitions.pernode', 100000), \\\n",
    "                           ('spark.app.name','Fits on HDFS')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    sc.stop()\n",
    "    spark.stop()\n",
    "    SparkSession._instantiatedContext = None\n",
    "\n",
    "    sc=SparkContext(conf=conf)\n",
    "    spark=SparkSession(sc)\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.visualization import astropy_mpl_style\n",
    "from astropy.wcs import WCS\n",
    "plt.style.use(astropy_mpl_style)\n",
    "\n",
    "## Let's run a simple object finder on our image,\n",
    "## and collect the catalog.\n",
    "import numpy as np\n",
    "from photutils import DAOStarFinder\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "from spectral_cube import SpectralCube\n",
    "from astroquery.esasky import ESASky\n",
    "from astroquery.utils import TableList\n",
    "from reproject import reproject_interp\n",
    "import astropy.units as u\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession, HiveContext\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.functions import randn, monotonically_increasing_id, row_number, desc, udf, col, lit\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import ArrayType, FloatType,IntegerType, DataType, DoubleType,Row, BooleanType\n",
    "\n",
    "\"\"\"\n",
    "Set the Spark datatypes\n",
    "\"\"\"\n",
    "iType=IntegerType()\n",
    "dType=DoubleType()\n",
    "fType=FloatType()\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector,VectorUDT\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, clock\n",
    "class Timer:\n",
    "    \"\"\"\n",
    "    a simple class for printing time (s) since last call\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0=time()\n",
    "        self.t1=clock()\n",
    "        self.elapsed = 1\n",
    "        self.elapsedCPU = 1\n",
    "    \n",
    "    def start(self):\n",
    "        self.t0=time()\n",
    "        self.t1=clock()\n",
    "        \n",
    "    def stop(self):\n",
    "        t1=time()\n",
    "        t2=clock()\n",
    "        print(\"Elapsed {:2.1f}s, CPU {:2.1f}s\".format(t1-self.t0, t2-self.t1))\n",
    "        self.elapsed = t1-self.t0\n",
    "        self.elapsedCPU = t2-self.t1\n",
    "\n",
    "class DataframeError(Exception): \n",
    "  \n",
    "    # Constructor or Initializer \n",
    "    def __init__(self, value): \n",
    "        self.value = value \n",
    "  \n",
    "    # __str__ is to print() the value \n",
    "    def __str__(self): \n",
    "        return(repr(self.value)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The CreateAndSaveArrays function saves the positional data (in this case, Right Ascension, Declination and Frequency) as an array element in a permanent Parquet table. the sparkfits_detail_arrays table is physically partitioned on the filename of the source FITS file and the array detail type (for example, 'RA -- SIN', 'DEC -- SIN ' and a spectra trype 'Hz', 'GHz' or 'km s-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAndSaveArrays(array, filename, arrayCoordinates, cubeSpectraUnits=None ):\n",
    "    if cubeSpectraUnits == None:\n",
    "        sda_detail_name=arrayCoordinates\n",
    "    else:\n",
    "        sda_detail_name=cubeSpectraUnits\n",
    "\n",
    "    #logger.in(filename, sda_detail_name )\n",
    "\n",
    "    arrayRDD=sc.parallelize(array).zipWithIndex().map(lambda x :(x[1], x[0]) )\n",
    "    arrayDF=arrayRDD.map(lambda x: (x[0], x[1].tolist()) ).toDF([\"index\", \"spectra\"])\n",
    "\n",
    "    #\"\"\"\n",
    "    #\n",
    "    # Maintaining order within a collect_list, See\n",
    "    # https://stackoverflow.com/questions/50766764/sorting-within-collect-list-in-hive\n",
    "    #\n",
    "    #\"\"\"\n",
    "\n",
    "    arrayDF.registerTempTable(\"SPECTRAS\")\n",
    "\n",
    "    sqlStmt=\"\"\"\n",
    "    with myData as\n",
    "    (\n",
    "        select 1 as grp,\n",
    "        index as idx,\n",
    "        map(\n",
    "            'spectraVal', spectra\n",
    "        ) as kv\n",
    "        from SPECTRAS\n",
    "        distribute by grp\n",
    "        sort by grp, index\n",
    "    )\n",
    "    select collect_list( float(a.kv['spectraVal']) ) as sda_detail_array \n",
    "    from myData a\n",
    "    group by grp\n",
    "    \"\"\"\n",
    "\n",
    "    spectraArrayDF=spark.sql(sqlStmt)\n",
    "    spectraArrayDF=spectraArrayDF.select(\"sda_detail_array\"\\\n",
    "                                         ,lit(filename).alias(\"sda_filename\")\\\n",
    "                                         ,lit(sda_detail_name).alias(\"sda_detail_type\"))\n",
    "\n",
    "    spectraArrayDF.registerTempTable(\"ARRAYS\")\n",
    "    spark.sql(\"\"\"\n",
    "    insert overwrite table fits_investigation.sparkfits_detail_arrays \n",
    "    partition(sda_filename , sda_detail_type) \n",
    "    select sda_detail_array, sda_filename, sda_detail_type\n",
    "    from ARRAYS\n",
    "    \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDimensions(cube, fileName, cubeDecCoordinate, cubeSpectraUnits, cubeDecDim):\n",
    "    \n",
    "    _, dec, _ = cube.world[0, :, 0]  #extract latitude world coordinates from cube dec\n",
    "    _, _, ra = cube.world[0, 0, :]  #extract longitude world coordinates from cube ra\n",
    "\n",
    "    \"\"\"\n",
    "    Make sure that we cast the Ra and Dec arrays explicitly to float32\n",
    "    \"\"\"\n",
    "    ra=np.array(ra, dtype=np.float32)\n",
    "    # and flip - always want declination ordered from greatest to lowest. eg -18 ..... -27 or 43 ..... 21\n",
    "    #if max(dec) <0 or 1==1:\n",
    "    \n",
    "    dec=np.flip( np.array(dec, dtype=np.float32) )\n",
    "    \n",
    "    #else:\n",
    "    #    dec=np.array(dec, dtype=np.float32)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Get the Spectral Axis data.\n",
    "    Note: Further enhancements can include changing the values of the Spectral axis - ie change from Hz to redshift or\n",
    "    velocity.\n",
    "    \n",
    "    \"\"\"\n",
    "    spectralAxis=cube.spectral_axis\n",
    "    \n",
    "    \"\"\"\n",
    "    Cast the SpectralCube explicitily to a numpy array, float 32\n",
    "    \"\"\"\n",
    "    \n",
    "    specArray=np.array(spectralAxis , dtype=np.float32)\n",
    "    \n",
    "    print(\"arrays created\")\n",
    "\n",
    "    return ra, dec, specArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and save the source FITS file header data to a Parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFITSHeaderData(header, fitsFilename):\n",
    "    hduArray=np.empty((0, 4))\n",
    "    Results=Row(\"fits_file\",\"index\",\"key\",\"value\",\"comment\")\n",
    "    for i in np.arange(len(header)):\n",
    "        theKey=list(header.keys())[int(i)]\n",
    "        theValue=header[int(i)]\n",
    "        theComment=header.comments[int(i)]\n",
    "\n",
    "        #print(fitsFilename, theKey, theValue, theComment)\n",
    "        if theKey and not theKey.isspace():\n",
    "            result=Results(fitsFilename, int(i), theKey, str(theValue), header.comments[int(i)])\n",
    "\n",
    "            if i == 0:\n",
    "                hduDF=spark.createDataFrame([result])\n",
    "            else:\n",
    "                newRow=spark.createDataFrame([result])\n",
    "                hduDF = hduDF.union(newRow)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    hduDF.printSchema()\n",
    "    hduDF.registerTempTable(\"HEADERS\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "    insert overwrite table fits_investigation.sparkfits_fits_headers partition(sfh_fits_file ) \n",
    "    select index, key, value, comment, fits_file from HEADERS\n",
    "    \"\"\")\n",
    "\n",
    "    print('Header data created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowdf_into_imagerdd(df, final_num_partition=1):\n",
    "    \"\"\"\n",
    "    Reshape a DataFrame of rows into a RDD containing the full image\n",
    "    in one partition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame of image rows.\n",
    "    final_num_partition : Int\n",
    "        The final number of partitions. Must be one (default) unless you\n",
    "        know what you are doing.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    imageRDD : RDD\n",
    "        RDD containing the full image in one partition\n",
    "        \n",
    "    NOTE: With larger FITS files, we need to avoid the use of .glom() as it results in larger memort usage.\n",
    "        From the documentation -\n",
    "        .glom(self) - Return an RDD created by coalescing all elements within each partition into a list.\n",
    "        \n",
    "    return df.rdd.zipWithIndex().coalesce(final_num_partition).glom()    \n",
    "    \"\"\"\n",
    "\n",
    "    return df.rdd.zipWithIndex() #.coalesce(final_num_partition).glom()\n",
    "\n",
    "def clean_image(im):\n",
    "    \"\"\"\n",
    "    Cleans out the Nan elements in the array\n",
    "    \"\"\"\n",
    "    #im=im[~np.isnan(im).any(axis=1)]\n",
    "    im = np.nan_to_num(im)\n",
    "    return im\n",
    "\n",
    "\n",
    "def replace_nan_with_float(im):\n",
    "    \"\"\"\n",
    "    Replaces Nan elements with a random float\n",
    "    \"\"\"\n",
    "    \n",
    "    #nan_mask = np.isnan( np.array(im, dtype=np.float32) )\n",
    "    nan_mask = np.isnan( np.array(im) )\n",
    "    \n",
    "    sampl = np.random.uniform(low=0.01, high=13.3, size=(im.shape))\n",
    "    \n",
    "    im[nan_mask] = sampl[nan_mask]\n",
    "    \n",
    "    #random_replace = np.vectorize(lambda x: np.random.random() if np.isnan(x) else x)\n",
    "    #im=random_replace(im)\n",
    "    return im\n",
    "    \n",
    "\n",
    "def calculateBand(x, bandCount):\n",
    "    x = x // bandCount\n",
    "    return x\n",
    "\n",
    "def CastStructToArray(StructCol):\n",
    "    imageArray = np.asarray(StructCol[0]).tolist()\n",
    "    return imageArray\n",
    "\n",
    "def calculateBand(x, bandCount=32):\n",
    "    x = x  // bandCount\n",
    "    return x\n",
    "\n",
    "def dummyReturn(x):\n",
    "    return x.tolist()\n",
    "\n",
    "def returnAsIs(x):\n",
    "    return x\n",
    "\n",
    "# ====================================================\n",
    "# Write results to Parquet table\n",
    "# ====================================================\n",
    "\n",
    "def writeResults(sc, resultDF, vMode, vFormat, vTable):\n",
    "    \n",
    "    resultDF.write.mode(vMode).format(vFormat).saveAsTable(vTable)\n",
    "    \n",
    "def GetCubePositionData(cube, fileName, cubeDecCoordinate, cubeSpectraUnits, cubeDecDim):\n",
    "    \n",
    "    _, dec, _ = cube.world[0, :, 0]  #extract latitude world coordinates from cube dec\n",
    "    _, _, ra = cube.world[0, 0, :]  #extract longitude world coordinates from cube ra\n",
    "\n",
    "    \"\"\"\n",
    "    Make sure that we cast the Ra and Dec arrays explicitly to float32\n",
    "    \"\"\"\n",
    "    ra=np.array(ra, dtype=np.float32)\n",
    "    # and flip - always want declination ordered from greatest to lowest. eg -18 ..... -27 or 43 ..... 21\n",
    "    #if max(dec) <0 or 1==1:\n",
    "    \n",
    "    dec=np.flip( np.array(dec, dtype=np.float32) )\n",
    "    \n",
    "    #else:\n",
    "    #    dec=np.array(dec, dtype=np.float32)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Get the Spectral Axis data.\n",
    "    Note: Further enhancements can include changing the values of the Spectral axis - ie change from Hz to redshift or\n",
    "    velocity.\n",
    "    \n",
    "    \"\"\"\n",
    "    spectralAxis=cube.spectral_axis\n",
    "    \n",
    "    \"\"\"\n",
    "    Cast the SpectralCube explicitily to a numpy array, float 32\n",
    "    \"\"\"\n",
    "    \n",
    "    specArray=np.array(spectralAxis , dtype=np.float32)\n",
    "    \n",
    "    print(\"arrays created\")\n",
    "    \"\"\" parallelize the arrays \n",
    "    \n",
    "    NOTE: \n",
    "    As spark does not necessarily maintain the order of the values in a join, we need to specifically define the ordering\n",
    "    in order to maintain the correct relationships between the position and spectra arrays we extract from te FITS cube via \n",
    "    SpectralCube during the population process.\n",
    "    \n",
    "    To do this, we create a parallelised array for the spectra and declinatino with a zip index. the cartesian join is then sorted \n",
    "    spectra and declination indexes, and then the index is created for the cartesian RDD.\n",
    "    \n",
    "    This ensures the proper sorting and ordering of the combined data.\n",
    "    \n",
    "    Note that this is a potentially an expensive computational operation on larget arrays, but as it is only a one-off\n",
    "    computation per file, this is acceptable.\n",
    "    \"\"\"\n",
    "    rdd1=sc.parallelize(specArray).zipWithIndex()\n",
    "    rdd2=sc.parallelize(dec).zipWithIndex()\n",
    "    \n",
    "    print(\"and parallelized\")\n",
    "    \n",
    "    \"\"\" Ordered cartesian join as a basis of the dataframe \"\"\"\n",
    "    fDecDF=rdd1.cartesian(rdd2)\\\n",
    "    .sortBy(lambda x:(x[0][1], x[1][1]), True)\\\n",
    "    .zipWithIndex()\\\n",
    "    .map(lambda x: (x[0][0][0].tolist(), x[0][1][0].tolist(), x[1]))\\\n",
    "    .toDF([\"spd_spectra_value\",\"spd_declination\", \"spd_index\"])\\\n",
    "    .select(\"spd_index\", \\\n",
    "                castToFloat(\"spd_spectra_value\").alias(\"spd_spectra_value\"), \\\n",
    "                castToFloat(\"spd_declination\").alias(\"spd_declination\"), \\\n",
    "                lit(fileName).alias(\"spd_filename\"), \\\n",
    "                lit(cubeDecCoordinate).alias(\"spd_position_type\"), \\\n",
    "                lit(cubeSpectraUnits).alias(\"spd_spectra_type\"), \\\n",
    "                GetBandUDF(\"spd_index\", lit(cubeDecDim)).alias(\"spd_band\")  )                \n",
    "    print(\"dataframe created\")\n",
    "    return ra, dec, specArray, fDecDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetBandUDF = udf(lambda arr, bw: calculateBand(np.int(arr), bw), iType)\n",
    "CastToInt = udf(lambda x: dummyReturn(x), iType)\n",
    "CastToFloatArrayUDF = udf(lambda x : CastStructToArray(x), ArrayType(fType))\n",
    "\n",
    "castToFloat = udf(lambda arr: returnAsIs(arr), fType)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next two cells are a chack your connextino through to your Hive database are correctly configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use fits_investigation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----------+\n",
      "|          database|           tableName|isTemporary|\n",
      "+------------------+--------------------+-----------+\n",
      "|fits_investigation|          array_test|      false|\n",
      "|fits_investigation|declimation_dim_temp|      false|\n",
      "|fits_investigation|    fits_header_data|      false|\n",
      "|fits_investigation|          float_test|      false|\n",
      "|fits_investigation|  imagetestsparkfits|      false|\n",
      "|fits_investigation|   localparalleltest|      false|\n",
      "|fits_investigation|localparalleltest...|      false|\n",
      "|fits_investigation|localparalleltest...|      false|\n",
      "|fits_investigation|localparalleltest...|      false|\n",
      "|fits_investigation|  newdfdeletemelater|      false|\n",
      "|fits_investigation|     parquetdataview|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "+------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the file\n",
    "\n",
    "At this stage, we still need access to the source FITS file on the local filesystem (or S3) as we'll be using the SpectralCube library to extract the position  and spectra arrays.\n",
    "\n",
    "We'll use the same file we used in Part 1.\n",
    "\n",
    "Obviously, in production ETL flows, these processes would be automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileName='image.restored.i.SB2338.V2.cube.fits'\n",
    "localFile='/mnt1/FITs/' + FileName\n",
    "hdfsFilePath='hdfs:///user/hduser/FITS_Files/image.restored.i.SB2338.V2.cube.fits' #+FileName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create the sparkfits source file address (was target File in Part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs:///user/hduser/FITS_Files/image.restored.i.SB2338.V2.cube2.parquet'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sourceFile=hdfsFilePath.replace('.fits', '2.parquet')\n",
    "sourceFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /mnt1/FITs/image.restored.i.SB2338.V2.cube.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU      77   (5607, 5654, 1, 2592)   float32   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could not memory map array with mode='readonly', falling back to mode='denywrite', which means that the array will be read-only [astropy.io.fits.file]\n",
      "WARNING: StokesWarning: Cube is a Stokes cube, returning spectral cube for I component [spectral_cube.spectral_cube]\n"
     ]
    }
   ],
   "source": [
    "timer=Timer()\n",
    "hdu=0\n",
    "df_parquet = spark.read.format('parquet').load(sourceFile)\n",
    "fits.info(\"/mnt1/FITs/image.restored.i.SB2338.V2.cube.fits\")\n",
    "hdu1=fits.open(\"/mnt1/FITs/image.restored.i.SB2338.V2.cube.fits\" , memmap=True)\n",
    "cubeHeader=hdu1[0].header\n",
    "cubeWCS=WCS(cubeHeader)\n",
    "cubeHz = SpectralCube.read(hdu1)\n",
    "\n",
    "hdu1.close()\n",
    "PartitionFileName = FileName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the header data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fits_file: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n",
      "Header data created\n"
     ]
    }
   ],
   "source": [
    "CreateFITSHeaderData(cubeHeader, PartitionFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image.restored.i.SB2338.V2.cube.fits'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PartitionFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarisation=cubeHeader[\"NAXIS3\"] # Polarisation 196608\n",
    "raLength=cubeHz.header[\"NAXIS1\"] # RA\n",
    "decLength=cubeHz.header[\"NAXIS2\"] # DEC\n",
    "spectraLength=cubeHz.header[\"NAXIS3\"] # Spectra (hx) Pixels \n",
    "\n",
    "numPolarisation=cubeHeader[\"NAXIS4\"]\n",
    "\n",
    "## Same as dec length, used in the GetBandUDF UDF functionsto calculate the polarisation and spectral band indexes.\n",
    "## See\n",
    "##        GetBandUDF(col(\"spi_index\"), lit(frequencyBandLength*spectraLength)).alias(\"spi_pol\"),\\\n",
    "##        GetBandUDF(col(\"spi_index\"), lit(frequencyBandLength)).alias(\"spi_band\") )\\\n",
    "\n",
    "\n",
    "frequencyBandLength=cubeHz.header[\"NAXIS2\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Sparkfits dataframe, and then populate the images table with the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get column names\n",
      "Create the dataframe with the index we'll use for declination\n",
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- Image: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- ImgIndex: long (nullable = true)\n",
      "\n",
      "Dataframe created\n",
      "Temporary raw data table created\n",
      "Elapsed 281.5s, CPU 0.5s\n"
     ]
    }
   ],
   "source": [
    "timer.start()\n",
    "print(\"Get column names\")\n",
    "cols = df_parquet.columns\n",
    "\n",
    "# Add the .zipWithIndex column\n",
    "print(\"Create the dataframe with the index we'll use for declination\")\n",
    "df = df_parquet.rdd.zipWithIndex().map(lambda row: (row[1],) + tuple(row[0])).toDF([\"index\"] + cols)\n",
    "df.printSchema()\n",
    "print(\"Dataframe created\")\n",
    "\n",
    "# Register the dataframe as a temporary table\n",
    "df.registerTempTable('RawDataWithIndex')\n",
    "print(\"Temporary raw data table created\")\n",
    "\n",
    "\n",
    "\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have to reset the raw data indexes\n",
    "\n",
    "Change the index values from 0 - <number of declinatin values within one channel>. In the case of CUBE1, 0 - 2592.\n",
    "    \n",
    "We do this by creating a temporary table containing the smallest index value for a specific image channel, which is identified by the ImgIndex field in the dataframe. The next cell demonstrates this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|ImgIndex|   loIdx|\n",
      "+--------+--------+\n",
      "|    1126| 5099356|\n",
      "|    2085| 6331880|\n",
      "|     347|  597934|\n",
      "|     416|13693377|\n",
      "|     906| 8507025|\n",
      "|    1779| 5690979|\n",
      "|     295| 7861176|\n",
      "|    2529|   73502|\n",
      "|    2305| 5707941|\n",
      "|     401|11980534|\n",
      "|    1589| 9165714|\n",
      "|     566|13089798|\n",
      "|     406| 1272711|\n",
      "|    1794| 5796771|\n",
      "|    2077| 2011673|\n",
      "|    1840|  134393|\n",
      "|     789| 3506066|\n",
      "|    2374| 9816598|\n",
      "|      49| 8562263|\n",
      "|     722|10381261|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "select ImgIndex, min(index) as loIdx \n",
    "from RawDataWithIndex \n",
    "group by ImgIndex\n",
    "\"\"\").registerTempTable(\"CORRECTOR\")\n",
    "\n",
    "sqlContext.sql(\"select * from CORRECTOR\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating the index adjustments. We'll do a Broadcast join on the Corrector table and create the new FITS file as temporary table \"NewImages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [index#1189L, image#1190, image.restored.i.SB2338.V2.cube.fits AS filename#1211, ImgIndex#1191L AS band#1212L, loIdx#1195L, (index#1189L - loIdx#1195L) AS new_index#1213L]\n",
      "+- *(5) SortMergeJoin [ImgIndex#1191L], [ImgIndex#1216L], Inner\n",
      "   :- *(2) Sort [ImgIndex#1191L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(ImgIndex#1191L, 144)\n",
      "   :     +- *(1) Filter isnotnull(ImgIndex#1191L)\n",
      "   :        +- Scan ExistingRDD[index#1189L,Image#1190,ImgIndex#1191L]\n",
      "   +- *(4) Sort [ImgIndex#1216L ASC NULLS FIRST], false, 0\n",
      "      +- *(4) HashAggregate(keys=[ImgIndex#1216L], functions=[min(index#1214L)])\n",
      "         +- Exchange hashpartitioning(ImgIndex#1216L, 144)\n",
      "            +- *(3) HashAggregate(keys=[ImgIndex#1216L], functions=[partial_min(index#1214L)])\n",
      "               +- *(3) Project [index#1214L, ImgIndex#1216L]\n",
      "                  +- *(3) Filter isnotnull(ImgIndex#1216L)\n",
      "                     +- Scan ExistingRDD[index#1214L,Image#1215,ImgIndex#1216L]\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    select /*+ BROADCAST(CORRECTOR) */\n",
    "        a.index,a.image,'{}' as filename, a.ImgIndex as band, b.loIdx,\n",
    "        (a.index-b.loIdx) as new_index\n",
    "    from RawDataWithIndex a\n",
    "        inner join CORRECTOR b\n",
    "            on a.ImgIndex=b.ImgIndex\n",
    "\"\"\".format(PartitionFileName, PartitionFileName)).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 0.0s, CPU 0.0s\n",
      "New images temp table created!\n",
      "+-------+--------------------+--------------------+----+-------+---------+\n",
      "|  index|               image|            filename|band|  loIdx|new_index|\n",
      "+-------+--------------------+--------------------+----+-------+---------+\n",
      "|7861176|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        0|\n",
      "|7861177|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        1|\n",
      "|7861178|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        2|\n",
      "|7861179|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        3|\n",
      "|7861180|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        4|\n",
      "|7861181|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        5|\n",
      "|7861182|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        6|\n",
      "|7861183|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        7|\n",
      "|7861184|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        8|\n",
      "|7861185|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|        9|\n",
      "|7861186|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       10|\n",
      "|7861187|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       11|\n",
      "|7861188|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       12|\n",
      "|7861189|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       13|\n",
      "|7861190|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       14|\n",
      "|7861191|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       15|\n",
      "|7861192|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       16|\n",
      "|7861193|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       17|\n",
      "|7861194|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       18|\n",
      "|7861195|[NaN, NaN, NaN, N...|image.restored.i....| 295|7861176|       19|\n",
      "+-------+--------------------+--------------------+----+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timer.start()\n",
    "sqlContext.sql(\"\"\"\n",
    "    select /*+ BROADCAST(CORRECTOR) */\n",
    "        a.index,a.image,'{}' as filename, a.ImgIndex as band, b.loIdx,\n",
    "        (a.index-b.loIdx) as new_index\n",
    "    from RawDataWithIndex a\n",
    "        inner join CORRECTOR b\n",
    "            on a.ImgIndex=b.ImgIndex\n",
    "\"\"\".format(PartitionFileName, PartitionFileName)).registerTempTable(\"NewImages\")\n",
    "timer.stop()\n",
    "\n",
    "print(\"New images temp table created!\")\n",
    "\n",
    "sqlContext.sql(\"select * from NewImages\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And writing the dataframe to the final parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Parquet...\n",
      "Done!\n",
      "Elapsed 1945.1s, CPU 1.3s\n"
     ]
    }
   ],
   "source": [
    "timer.start()\n",
    "print(\"Writing to Parquet...\")\n",
    "sqlContext.sql(\"\"\"\n",
    "insert overwrite table sparkfits_images partition(spi_filename, spi_band )\n",
    "select new_index, image, '{}', band from NewImages\n",
    "\"\"\".format(PartitionFileName))\n",
    "print(\"Done!\")\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dimensions and populate the dimension table\n",
    "\n",
    "First, we create the cubes with the alternative positions and/or spectra values we're interested in.\n",
    "\n",
    "In this example, we create two new SpectralCubes with spectral units of GHz and velocity in km s-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create the alternative spectral cubes for the differing spectral arrays\n",
      "Alternative cubes created\n",
      "Elapsed 0.1s, CPU 0.1s\n"
     ]
    }
   ],
   "source": [
    "timer.start()\n",
    "print(\"Create the alternative spectral cubes for the differing spectral arrays\")\n",
    "cubeVel = cubeHz.with_spectral_unit(u.km / u.s, velocity_convention='radio' ,rest_value=200 * u.Hz)\n",
    "cubeGHz = cubeHz.with_spectral_unit(u.GHz, velocity_convention='radio' ,rest_value=200 * u.GHz)\n",
    "print(\"Alternative cubes created\")\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we extract the data and write the dimensions to the dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASKAPSoft\n",
      "beam-1 Jy FREQ Hz\n",
      "arrays created\n",
      "Creating ra and dec detail arrays\n",
      "Creating the spectral detail array\n",
      "Elapsed 32.2s, CPU 0.2s\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate([cubeHz]):\n",
    "    timer.start()\n",
    "    print(c.header[\"ORIGIN\"])\n",
    "\n",
    "    cubePixelValue = c.header[\"BUNIT\"]\n",
    "    cubeRaDim = c.header[\"NAXIS1\"]\n",
    "    cubeDecDim = c.header[\"NAXIS2\"]\n",
    "    cubeSpectraDim = c.header[\"NAXIS3\"]\n",
    "    cubeRACoordinate = c.header[\"CTYPE1\"]\n",
    "    cubeDecCoordinate = c.header[\"CTYPE2\"]\n",
    "    cubeSpectraCoordinate = c.header[\"CTYPE3\"]\n",
    "    cubeRAUnits = c.header[\"CUNIT1\"]\n",
    "    cubeDecUnits = c.header[\"CUNIT2\"]\n",
    "    cubeSpectraUnits = c.header[\"CUNIT3\"]\n",
    "    print(cubePixelValue, cubeSpectraCoordinate, cubeSpectraUnits)\n",
    "    timer.start()\n",
    "\n",
    "    ra, dec, specArray = GetDimensions(c, PartitionFileName, cubeDecCoordinate, cubeSpectraUnits, cubeDecDim)\n",
    "    #ra, dec, specArray, DecDF = FastGetCubePositions(c, PartitionFileName, cubeDecCoordinate, cubeSpectraUnits, cubeDecDim)\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        print(\"Creating ra and dec detail arrays\")\n",
    "        CreateAndSaveArrays(ra,  PartitionFileName, cubeRACoordinate)\n",
    "        CreateAndSaveArrays(dec, PartitionFileName, cubeDecCoordinate)\n",
    "\n",
    "    print(\"Creating the spectral detail array\")\n",
    "    CreateAndSaveArrays(specArray, PartitionFileName, cubeSpectraCoordinate, cubeSpectraUnits) \n",
    "    timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done and dusted!\n"
     ]
    }
   ],
   "source": [
    "print(\"All done and dusted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas and SparkFits (Spark 2.3.0, python 3.6, increased resources)",
   "language": "python",
   "name": "sparkfits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
