{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library import\n",
    "# ====================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.visualization import astropy_mpl_style\n",
    "from astropy.wcs import WCS\n",
    "plt.style.use(astropy_mpl_style)\n",
    "\n",
    "## Let's run a simple object finder on our image,\n",
    "## and collect the catalog.\n",
    "import numpy as np\n",
    "from photutils import DAOStarFinder\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "from spectral_cube import SpectralCube\n",
    "from astroquery.esasky import ESASky\n",
    "from astroquery.utils import TableList\n",
    "from reproject import reproject_interp\n",
    "import astropy.units as u\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession, HiveContext\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.functions import randn, monotonically_increasing_id, row_number, desc, udf, col, lit\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import ArrayType, FloatType,IntegerType, DataType, DoubleType,Row, BooleanType\n",
    "\n",
    "\"\"\"\n",
    "Set the Spark datatypes\n",
    "\"\"\"\n",
    "iType=IntegerType()\n",
    "dType=DoubleType()\n",
    "fType=FloatType()\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector,VectorUDT\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Spark Context\n",
    "\n",
    "For these processes, we need to change the spark context parameters because we're using this notebook to dynamically insert data into Hive partitioned tables; this means we need to set the hive.exec.dynamic parameters. See the context configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([('spark.executor.memory', '8g'),\\\n",
    "                           ('spark.driver.memory', '6g'),\\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.sql.shuffle.partitions', 144), \\\n",
    "                           ('spark.default.parallelism', 144),\\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.executor.instances', 40), \\\n",
    "                           ('spark.local.dir','/mnt/FITs/Spark/tmp'),\\\n",
    "                           ('spark.executor.memoryOverhead', '6g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.rpc.message.maxSize', '512'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '1g'),\\\n",
    "                           ('spark.hadoop.hive.exec.dynamic.partition', True), \\\n",
    "                           ('spark.hadoop.hive.exec.dynamic.partition.mode','nonstrict'), \\\n",
    "                           ('spark.hadoop.hive.exec.max.dynamic.partitions', 100000), \\\n",
    "                           ('spark.hadoop.hive.exec.max.dynamic.partitions.pernode', 100000), \\\n",
    "                           ('spark.app.name','Fits on HDFS')])\n",
    "\n",
    "    # Stop the old context\n",
    "    #sc.stop()\n",
    "\n",
    "    # And restart the context with the new configuration\n",
    "    #sc = SparkContext(conf=conf)\n",
    "    #sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f0b12166080>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, clock\n",
    "class Timer:\n",
    "    \"\"\"\n",
    "    a simple class for printing time (s) since last call\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0=time()\n",
    "        self.t1=clock()\n",
    "        self.elapsed = 1\n",
    "        self.elapsedCPU = 1\n",
    "    \n",
    "    def start(self):\n",
    "        self.t0=time()\n",
    "        self.t1=clock()\n",
    "        \n",
    "    def stop(self):\n",
    "        t1=time()\n",
    "        t2=clock()\n",
    "        print(\"Elapsed {:2.1f}s, CPU {:2.1f}s\".format(t1-self.t0, t2-self.t1))\n",
    "        self.elapsed = t1-self.t0\n",
    "        self.elapsedCPU = t2-self.t1\n",
    "\n",
    "class DataframeError(Exception): \n",
    "  \n",
    "    # Constructor or Initializer \n",
    "    def __init__(self, value): \n",
    "        self.value = value \n",
    "  \n",
    "    # __str__ is to print() the value \n",
    "    def __str__(self): \n",
    "        return(repr(self.value)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The CreateAndSaveArrays function saves the positional data (in this case, Right Ascencion, Declination and Frequency) as an array element in a permanent Parquet table. the sparkfits_detail_arrays table is physically partitioned on the filename of the source FITS file and the array detail type (for example, 'RA -- SIN', 'DEC -- SIN ' and a spectra trype 'Hz', 'GHz' or 'km s-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAndSaveArrays(array, filename, arrayCoordinates, cubeSpectraUnits=None ):\n",
    "    if cubeSpectraUnits == None:\n",
    "        sda_detail_name=arrayCoordinates\n",
    "    else:\n",
    "        sda_detail_name=cubeSpectraUnits\n",
    "\n",
    "    #logger.in(filename, sda_detail_name )\n",
    "\n",
    "    arrayRDD=sc.parallelize(array).zipWithIndex().map(lambda x :(x[1], x[0]) )\n",
    "    arrayDF=arrayRDD.map(lambda x: (x[0], x[1].tolist()) ).toDF([\"index\", \"spectra\"])\n",
    "\n",
    "    #\"\"\"\n",
    "    #\n",
    "    # Maintaining order within a collect_list, See\n",
    "    # https://stackoverflow.com/questions/50766764/sorting-within-collect-list-in-hive\n",
    "    #\n",
    "    #\"\"\"\n",
    "\n",
    "    arrayDF.registerTempTable(\"SPECTRAS\")\n",
    "\n",
    "    sqlStmt=\"\"\"\n",
    "    with myData as\n",
    "    (\n",
    "        select 1 as grp,\n",
    "        index as idx,\n",
    "        map(\n",
    "            'spectraVal', spectra\n",
    "        ) as kv\n",
    "        from SPECTRAS\n",
    "        distribute by grp\n",
    "        sort by grp, index\n",
    "    )\n",
    "    select collect_list( float(a.kv['spectraVal']) ) as sda_detail_array \n",
    "    from myData a\n",
    "    group by grp\n",
    "    \"\"\"\n",
    "\n",
    "    spectraArrayDF=sqlContext.sql(sqlStmt)\n",
    "    spectraArrayDF=spectraArrayDF.select(\"sda_detail_array\"\\\n",
    "                                         ,lit(filename).alias(\"sda_filename\")\\\n",
    "                                         ,lit(sda_detail_name).alias(\"sda_detail_type\"))\n",
    "\n",
    "    spectraArrayDF.registerTempTable(\"ARRAYS\")\n",
    "    sqlContext.sql(\"\"\"\n",
    "    insert overwrite table sparkfits_detail_arrays \n",
    "    partition(sda_filename , sda_detail_type) \n",
    "    select sda_detail_array, sda_filename, sda_detail_type\n",
    "    from ARRAYS\n",
    "    \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowdf_into_imagerdd(df, final_num_partition=1):\n",
    "    \"\"\"\n",
    "    Reshape a DataFrame of rows into a RDD containing the full image\n",
    "    in one partition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame of image rows.\n",
    "    final_num_partition : Int\n",
    "        The final number of partitions. Must be one (default) unless you\n",
    "        know what you are doing.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    imageRDD : RDD\n",
    "        RDD containing the full image in one partition\n",
    "        \n",
    "    NOTE: With larger FITS files, we need to avoid the use of .glom() as it results in larger memort usage.\n",
    "        From the documentation -\n",
    "        .glom(self) - Return an RDD created by coalescing all elements within each partition into a list.\n",
    "        \n",
    "    return df.rdd.zipWithIndex().coalesce(final_num_partition).glom()    \n",
    "    \"\"\"\n",
    "\n",
    "    return df.rdd.zipWithIndex() #.coalesce(final_num_partition).glom()\n",
    "\n",
    "def clean_image(im):\n",
    "    \"\"\"\n",
    "    Cleans out the Nan elements in the array\n",
    "    \"\"\"\n",
    "    #im=im[~np.isnan(im).any(axis=1)]\n",
    "    im = np.nan_to_num(im)\n",
    "    return im\n",
    "\n",
    "\n",
    "def replace_nan_with_float(im):\n",
    "    \"\"\"\n",
    "    Replaces Nan elements with a random float\n",
    "    \"\"\"\n",
    "    \n",
    "    #nan_mask = np.isnan( np.array(im, dtype=np.float32) )\n",
    "    nan_mask = np.isnan( np.array(im) )\n",
    "    \n",
    "    sampl = np.random.uniform(low=0.01, high=13.3, size=(im.shape))\n",
    "    \n",
    "    im[nan_mask] = sampl[nan_mask]\n",
    "    \n",
    "    #random_replace = np.vectorize(lambda x: np.random.random() if np.isnan(x) else x)\n",
    "    #im=random_replace(im)\n",
    "    return im\n",
    "    \n",
    "\n",
    "def calculateBand(x, bandCount):\n",
    "    x = x // bandCount\n",
    "    return x\n",
    "\n",
    "def CastStructToArray(StructCol):\n",
    "    imageArray = np.asarray(StructCol[0]).tolist()\n",
    "    return imageArray\n",
    "\n",
    "def calculateBand(x, bandCount=32):\n",
    "    x = x  // bandCount\n",
    "    return x\n",
    "\n",
    "def dummyReturn(x):\n",
    "    return x.tolist()\n",
    "\n",
    "def returnAsIs(x):\n",
    "    return x\n",
    "\n",
    "# ====================================================\n",
    "# Write results to Parquet table\n",
    "# ====================================================\n",
    "\n",
    "def writeResults(sc, resultDF, vMode, vFormat, vTable):\n",
    "    \n",
    "    resultDF.write.mode(vMode).format(vFormat).saveAsTable(vTable)\n",
    "    \n",
    "def GetCubePositionData(cube, fileName, cubeDecCoordinate, cubeSpectraUnits, cubeDecDim):\n",
    "    \n",
    "    _, dec, _ = cube.world[0, :, 0]  #extract latitude world coordinates from cube dec\n",
    "    _, _, ra = cube.world[0, 0, :]  #extract longitude world coordinates from cube ra\n",
    "\n",
    "    \"\"\"\n",
    "    Make sure that we cast the Ra and Dec arrays explicitly to float32\n",
    "    \"\"\"\n",
    "    ra=np.array(ra, dtype=np.float32)\n",
    "    # and flip - always want declination ordered from greatest to lowest. eg -18 ..... -27 or 43 ..... 21\n",
    "    #if max(dec) <0 or 1==1:\n",
    "    \n",
    "    dec=np.flip( np.array(dec, dtype=np.float32) )\n",
    "    \n",
    "    #else:\n",
    "    #    dec=np.array(dec, dtype=np.float32)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Get the Spectral Axis data.\n",
    "    Note: Further enhancements can include changing the values of the Spectral axis - ie change from Hz to redshift or\n",
    "    velocity.\n",
    "    \n",
    "    \"\"\"\n",
    "    spectralAxis=cube.spectral_axis\n",
    "    \n",
    "    \"\"\"\n",
    "    Cast the SpectralCube explicitily to a numpy array, float 32\n",
    "    \"\"\"\n",
    "    \n",
    "    specArray=np.array(spectralAxis , dtype=np.float32)\n",
    "    \n",
    "    print(\"arrays created\")\n",
    "    \"\"\" parallelize the arrays \n",
    "    \n",
    "    NOTE: \n",
    "    As spark does not necessarily maintain the order of the values in a join, we need to specifically define the ordering\n",
    "    in order to maintain the correct relationships between the position and spectra arrays we extract from te FITS cube via \n",
    "    SpectralCube during the population process.\n",
    "    \n",
    "    To do this, we create a parallelised array for the spectra and declinatino with a zip index. the cartesian join is then sorted \n",
    "    spectra and declination indexes, and then the index is created for the cartesian RDD.\n",
    "    \n",
    "    This ensures the proper sorting and ordering of the combined data.\n",
    "    \n",
    "    Note that this is a potentially an expensive computational operation on larget arrays, but as it is only a one-off\n",
    "    computation per file, this is acceptable.\n",
    "    \"\"\"\n",
    "    rdd1=sc.parallelize(specArray).zipWithIndex()\n",
    "    rdd2=sc.parallelize(dec).zipWithIndex()\n",
    "    \n",
    "    print(\"and parallelized\")\n",
    "    \n",
    "    \"\"\" Ordered cartesian join as a basis of the dataframe \"\"\"\n",
    "    fDecDF=rdd1.cartesian(rdd2)\\\n",
    "    .sortBy(lambda x:(x[0][1], x[1][1]), True)\\\n",
    "    .zipWithIndex()\\\n",
    "    .map(lambda x: (x[0][0][0].tolist(), x[0][1][0].tolist(), x[1]))\\\n",
    "    .toDF([\"spd_spectra_value\",\"spd_declination\", \"spd_index\"])\\\n",
    "    .select(\"spd_index\", \\\n",
    "                castToFloat(\"spd_spectra_value\").alias(\"spd_spectra_value\"), \\\n",
    "                castToFloat(\"spd_declination\").alias(\"spd_declination\"), \\\n",
    "                lit(fileName).alias(\"spd_filename\"), \\\n",
    "                lit(cubeDecCoordinate).alias(\"spd_position_type\"), \\\n",
    "                lit(cubeSpectraUnits).alias(\"spd_spectra_type\"), \\\n",
    "                GetBandUDF(\"spd_index\", lit(cubeDecDim)).alias(\"spd_band\")  )                \n",
    "    print(\"dataframe created\")\n",
    "    return ra, dec, specArray, fDecDF\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "row, col = 10, 5\n",
    "matrix = np.random.randint(2, size=(row,col))\n",
    "matrix = matrix.astype(float)\n",
    "matrix[1,2] = np.nan\n",
    "matrix[5,3] = np.nan\n",
    "matrix[8,0] = np.nan\n",
    "\n",
    "print(matrix)\n",
    "\n",
    "pmatrix=sc.parallelize(matrix)\n",
    "\n",
    "pmatrix.take(10)\n",
    "\n",
    "pmatrix=pmatrix.map(lambda x: replace_nan_with_float(np.array(x) ))\n",
    "\n",
    "pmatrix.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDF Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetBandUDF = udf(lambda arr, bw: calculateBand(np.int(arr), bw), iType)\n",
    "CastToInt = udf(lambda x: dummyReturn(x), iType)\n",
    "CastToFloatArrayUDF = udf(lambda x : CastStructToArray(x), ArrayType(fType))\n",
    "\n",
    "castToFloat = udf(lambda arr: returnAsIs(arr), fType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use fits_investigation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----------+\n",
      "|          database|           tableName|isTemporary|\n",
      "+------------------+--------------------+-----------+\n",
      "|fits_investigation|          array_test|      false|\n",
      "|fits_investigation|    fits_header_data|      false|\n",
      "|fits_investigation|          float_test|      false|\n",
      "|fits_investigation|  imagetestsparkfits|      false|\n",
      "|fits_investigation|   localparalleltest|      false|\n",
      "|fits_investigation|localparalleltest...|      false|\n",
      "|fits_investigation|localparalleltest...|      false|\n",
      "|fits_investigation|localparalleltest...|      false|\n",
      "|fits_investigation|  newdfdeletemelater|      false|\n",
      "|fits_investigation|     parquetdataview|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetunpartitio...|      false|\n",
      "+------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "UseResidual=False\n",
    "\n",
    "if UseResidual:\n",
    "    bigFilePath='hdfs:///user/hduser/FITS_Files/residual.i.SB8170.cube.fits'\n",
    "    bigFileName=\"residual.i.SB8170.cube.fits\"\n",
    "    localBigFile=\"/mnt/FITs/residual.i.SB8170.cube.fits\"\n",
    "else:\n",
    "    bigFilePath='hdfs:///user/hduser/FITS_Files/image.restored.i.SB2338.V2.cube.fits'\n",
    "    bigFileName=\"image.restored.i.SB2338.V2.cube.fits\"\n",
    "    localBigFile=\"/mnt1/FITs/image.restored.i.SB2338.V2.cube.fits\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /mnt1/FITs/image.restored.i.SB2338.V2.cube.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU      77   (5607, 5654, 1, 2592)   float32   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could not memory map array with mode='readonly', falling back to mode='denywrite', which means that the array will be read-only [astropy.io.fits.file]\n",
      "WARNING: StokesWarning: Cube is a Stokes cube, returning spectral cube for I component [spectral_cube.spectral_cube]\n"
     ]
    }
   ],
   "source": [
    "timer=Timer()\n",
    "hdu=0\n",
    "df = sqlContext.read.format(\"fits\").option(\"hdu\", hdu).load(bigFilePath)\n",
    "fits.info(localBigFile)\n",
    "hdu1=fits.open(localBigFile , memmap=True)\n",
    "cubeHeader=hdu1[0].header\n",
    "cubeWCS=WCS(cubeHeader)\n",
    "cubeHz = SpectralCube.read(hdu1)\n",
    "\n",
    "hdu1.close()\n",
    "PartitionFileName = bigFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image.restored.i.SB2338.V2.cube.fits'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PartitionFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarisation=cubeHeader[\"NAXIS3\"] # Polarisation 196608\n",
    "raLength=cubeHz.header[\"NAXIS1\"] # RA\n",
    "decLength=cubeHz.header[\"NAXIS2\"] # DEC\n",
    "spectraLength=cubeHz.header[\"NAXIS3\"] # Spectra (hx) Pixels \n",
    "\n",
    "numPolarisation=cubeHeader[\"NAXIS4\"]\n",
    "\n",
    "## Same as dec length, used in the GetBandUDF UDF functionsto calculate the polarisation and spectral band indexes.\n",
    "## See\n",
    "##        GetBandUDF(col(\"spi_index\"), lit(frequencyBandLength*spectraLength)).alias(\"spi_pol\"),\\\n",
    "##        GetBandUDF(col(\"spi_index\"), lit(frequencyBandLength)).alias(\"spi_band\") )\\\n",
    "\n",
    "\n",
    "frequencyBandLength=cubeHz.header[\"NAXIS2\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Sparkfits dataframe, and then populate the images table with the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file partition numbers - 6677\n",
      "RDD with zip index created\n",
      "Elapsed 1508.7s, CPU 1.3s\n",
      "Replacing NaN values with random float\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 38288, hercules-2-2.nimbus.pawsey.org.au, executor 24): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/rdd.py\", line 1352, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-96-1dbb444be03f>\", line 17, in <lambda>\n  File \"<ipython-input-94-7e099962565d>\", line 47, in replace_nan_with_float\nIndexError: too many indices for array\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/rdd.py\", line 1352, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-96-1dbb444be03f>\", line 17, in <lambda>\n  File \"<ipython-input-94-7e099962565d>\", line 47, in replace_nan_with_float\nIndexError: too many indices for array\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-1dbb444be03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Replacing NaN values with random float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mPartitionFileName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPartitionFileName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.NaNs.Removed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mimageDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimageRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mreplace_nan_with_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"spi_image\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"spi_index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mimageDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimageRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"spi_image\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"spi_index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \"\"\"\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \"\"\"\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \"\"\"\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 38288, hercules-2-2.nimbus.pawsey.org.au, executor 24): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/rdd.py\", line 1352, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-96-1dbb444be03f>\", line 17, in <lambda>\n  File \"<ipython-input-94-7e099962565d>\", line 47, in replace_nan_with_float\nIndexError: too many indices for array\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/rdd.py\", line 1352, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-96-1dbb444be03f>\", line 17, in <lambda>\n  File \"<ipython-input-94-7e099962565d>\", line 47, in replace_nan_with_float\nIndexError: too many indices for array\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "timer.start()\n",
    "partitionTarget=df.rdd.getNumPartitions()\n",
    "print(\"Source file partition numbers - {}\".format(partitionTarget))\n",
    "\n",
    "### Create the RDD with the zip index\n",
    "imageRDD=rowdf_into_imagerdd(df)\n",
    "\n",
    "print(\"RDD with zip index created\")\n",
    "timer.stop()\n",
    "\n",
    "timer.start()\n",
    "bRemoveNaNs=False\n",
    "\n",
    "if bRemoveNaNs:\n",
    "    print(\"Replacing NaN values with random float\")\n",
    "    PartitionFileName=PartitionFileName+'.NaNs.Removed'\n",
    "    imageDF=imageRDD.map(lambda x: ( replace_nan_with_float(np.array(x[0]) ).tolist(), x[1] ) ).toDF([\"spi_image\",\"spi_index\"])\n",
    "else:\n",
    "    imageDF=imageRDD.toDF([\"spi_image\",\"spi_index\"])\n",
    "\n",
    "\n",
    "numPartitions = imageDF.rdd.getNumPartitions()\n",
    "\n",
    "if numPartitions < partitionTarget:\n",
    "    imageDF=imageDF.repartition(partitionTarget)\n",
    "    print(\"Repartitioning the dataframe from {} to {} partitions...\".format(numPartitions, partitionTarget))\n",
    "\n",
    "dff=imageDF.select(\\\n",
    "                   \"spi_index\", \\\n",
    "                   CastToFloatArrayUDF(\"spi_image\").alias(\"spi_image\"), \\\n",
    "                   lit(PartitionFileName).alias(\"spi_filename\"), \\\n",
    "                   GetBandUDF(\"spi_index\", lit(frequencyBandLength*spectraLength)).alias(\"spi_pol\"),\\\n",
    "                   GetBandUDF(\"spi_index\", lit(frequencyBandLength)).alias(\"spi_band\") )\\\n",
    ".persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "print(\"Dataframe created and persisted\")\n",
    "\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spi_index: long (nullable = true)\n",
      " |-- spi_image: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- spi_filename: string (nullable = false)\n",
      " |-- spi_pol: integer (nullable = true)\n",
      " |-- spi_band: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.registerTempTable(\"IMAGES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------+\n",
      "|spi_index|           spi_image|        spi_filename|spi_band|\n",
      "+---------+--------------------+--------------------+--------+\n",
      "|        0|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|        1|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|        2|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|        3|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|        4|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|        5|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|        6|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|        7|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|        8|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|        9|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       10|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       11|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       12|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       13|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       14|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       15|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       16|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       17|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       18|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "|       19|[0.0, 0.0, 0.0, 0...|residual.i.SB8170...|       0|\n",
      "+---------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select spi_index, spi_image, spi_filename, spi_band from IMAGES\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the dataframe into the target table.\n",
    "\n",
    "Check the polarisations. If there is only one polarisation, we can ignore that and insert into the sparkfits_images table; otherwise we insert into sparkfits_polarised_images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataframe to parquet images table\n",
      "Saving to sparkfits_images\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing dataframe to parquet images table\")\n",
    "timer.start()\n",
    "if polarisation==1:\n",
    "    print(\"Saving to sparkfits_images\")\n",
    "    sqlContext.sql(\"\"\"\n",
    "    insert overwrite table sparkfits_images partition(spi_filename, spi_band ) \n",
    "    select spi_index, spi_image, spi_filename, spi_band from IMAGES\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"Saving to sparkfits_polarised_images\")\n",
    "    sqlContext.sql(\"\"\"\n",
    "    insert overwrite table sparkfits_polarised_images partition(spi_filename, spi_pol, spi_band ) \n",
    "    select spi_index, spi_image, spi_filename, spi_pol, spi_band from IMAGES\n",
    "    \"\"\")\n",
    "\n",
    "print(\"All done!\")\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Table Population\n",
    "\n",
    "The example below create dimension tables for cubes with differing spectral units. We create two new SpectralCube objects with velocity and GHz spectra,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the dimension tables\n",
      "Create the alternative spectral cubes for the differing spectral arrays\n",
      "Alternative cubes created\n",
      "Elapsed 0.1s, CPU 0.1s\n"
     ]
    }
   ],
   "source": [
    "print(\"Populating the dimension tables\")\n",
    "\n",
    "timer.start()\n",
    "print(\"Create the alternative spectral cubes for the differing spectral arrays\")\n",
    "cubeVel = cubeHz.with_spectral_unit(u.km / u.s, velocity_convention='radio' ,rest_value=200 * u.Hz)\n",
    "cubeGHz = cubeHz.with_spectral_unit(u.GHz, velocity_convention='radio' ,rest_value=200 * u.GHz)\n",
    "print(\"Alternative cubes created\")\n",
    "timer.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASKAPSoft\n",
      "beam-1 Jy VRAD km s-1\n",
      "arrays created\n",
      "and parallelized\n",
      "dataframe created\n",
      "Creating the temp table POSDIM and writing the data into the dimension table\n",
      "Creating ra and dec detail arrays\n",
      "Creating the spectral detail array\n",
      "Elapsed 552.1s, CPU 5.4s\n",
      "ASKAPSoft\n",
      "beam-1 Jy VRAD km s-1\n",
      "arrays created\n",
      "and parallelized\n",
      "dataframe created\n",
      "Creating the temp table POSDIM and writing the data into the dimension table\n",
      "Creating the spectral detail array\n",
      "Elapsed 563.4s, CPU 5.3s\n",
      "ASKAPSoft\n",
      "beam-1 Jy FREQ GHz\n",
      "arrays created\n",
      "and parallelized\n",
      "dataframe created\n",
      "Creating the temp table POSDIM and writing the data into the dimension table\n",
      "Creating the spectral detail array\n",
      "Elapsed 547.2s, CPU 5.2s\n"
     ]
    }
   ],
   "source": [
    "#for i, c in enumerate([cubeHz, cubeVel, cubeGHz]):\n",
    "for i, c in enumerate([cubeVel, cubeVel, cubeGHz]):\n",
    "    timer.start()\n",
    "    print(c.header[\"ORIGIN\"])\n",
    "\n",
    "    cubePixelValue = c.header[\"BUNIT\"]\n",
    "    cubeRaDim = c.header[\"NAXIS1\"]\n",
    "    cubeDecDim = c.header[\"NAXIS2\"]\n",
    "    cubeSpectraDim = c.header[\"NAXIS3\"]\n",
    "    cubeRACoordinate = c.header[\"CTYPE1\"]\n",
    "    cubeDecCoordinate = c.header[\"CTYPE2\"]\n",
    "    cubeSpectraCoordinate = c.header[\"CTYPE3\"]\n",
    "    cubeRAUnits = c.header[\"CUNIT1\"]\n",
    "    cubeDecUnits = c.header[\"CUNIT2\"]\n",
    "    cubeSpectraUnits = c.header[\"CUNIT3\"]\n",
    "    print(cubePixelValue, cubeSpectraCoordinate, cubeSpectraUnits)\n",
    "    timer.start()\n",
    "\n",
    "    ra, dec, specArray, DecDF = GetCubePositionData(c, PartitionFileName, cubeDecCoordinate, cubeSpectraUnits, cubeDecDim)\n",
    "\n",
    "    \"\"\"\n",
    "    Add the declination and frequency dimension to the table\n",
    "    \"\"\"\n",
    "    print(\"Creating the temp table POSDIM and writing the data into the dimension table\")\n",
    "\n",
    "    DecDF.registerTempTable(\"POSDIM\")\n",
    "    \n",
    "    newPosDimDF=sqlContext.sql(\"\"\"\n",
    "        with myData as\n",
    "        (\n",
    "        select\n",
    "        spd_filename,spd_position_type,spd_spectra_type,\n",
    "        spd_declination,\n",
    "            map(\n",
    "                'idxVal', spd_index\n",
    "            ) as kv\n",
    "            from POSDIM\n",
    "            distribute by spd_declination\n",
    "            sort by spd_index\n",
    "        )\n",
    "        select\n",
    "        spd_filename,spd_position_type,spd_spectra_type,\n",
    "        spd_declination,\n",
    "        collect_list( float(a.kv['idxVal']) ) as spd_declination_spectra \n",
    "        from myData a\n",
    "        group by spd_filename,spd_position_type,spd_spectra_type,spd_declination\n",
    "        order by spd_declination desc\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    newPosDimDF.registerTempTable('NewPosDim')\n",
    "\n",
    "    sqlContext.sql(\"\"\"\n",
    "    insert overwrite table sparkfits_new_position_dimension\n",
    "    partition(spd_filename, spd_position_type, spd_spectra_type) \n",
    "    select spd_declination,spd_declination_spectra,\n",
    "    spd_filename,spd_position_type,spd_spectra_type\n",
    "    from NewPosDim\n",
    "    \"\"\")\n",
    "    \n",
    "    if False:\n",
    "        sqlContext.sql(\"\"\"\n",
    "        insert overwrite table sparkfits_position_dimension \n",
    "        partition(spd_filename, spd_position_type, spd_spectra_type, spd_band) \n",
    "        select spd_index, spd_spectra_value, spd_declination,\n",
    "        spd_filename,spd_position_type,spd_spectra_type,spd_band\n",
    "        from POSDIM\n",
    "        \"\"\")\n",
    "\n",
    "    if i == 0:\n",
    "        print(\"Creating ra and dec detail arrays\")\n",
    "        CreateAndSaveArrays(ra,  PartitionFileName, cubeRACoordinate)\n",
    "        CreateAndSaveArrays(dec, PartitionFileName, cubeDecCoordinate)\n",
    "\n",
    "    print(\"Creating the spectral detail array\")\n",
    "    CreateAndSaveArrays(specArray, PartitionFileName, cubeSpectraCoordinate, cubeSpectraUnits) \n",
    "    timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas and SparkFits (Spark 2.3.0, python 3.6, increased resources)",
   "language": "python",
   "name": "sparkfits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
