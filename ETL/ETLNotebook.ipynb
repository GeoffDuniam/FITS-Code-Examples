{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Extract Transform and Load (ETL) procedure\n",
    "\n",
    "This notebook will walk you through the process that loads the data from a FITS cube into the Parquet tables on a Hive enables HDFS cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library import\n",
    "# ====================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.visualization import astropy_mpl_style\n",
    "from astropy.wcs import WCS\n",
    "plt.style.use(astropy_mpl_style)\n",
    "\n",
    "## Let's run a simple object finder on our image,\n",
    "## and collect the catalog.\n",
    "import numpy as np\n",
    "from photutils import DAOStarFinder\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "from spectral_cube import SpectralCube\n",
    "from astroquery.esasky import ESASky\n",
    "from astroquery.utils import TableList\n",
    "from reproject import reproject_interp\n",
    "import astropy.units as u\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession, HiveContext\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.functions import randn, monotonically_increasing_id, row_number, desc, udf, col, lit\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import ArrayType, FloatType,IntegerType, DataType, DoubleType,Row, BooleanType\n",
    "\n",
    "\"\"\"\n",
    "Set the Spark datatypes\n",
    "\"\"\"\n",
    "iType=IntegerType()\n",
    "dType=DoubleType()\n",
    "fType=FloatType()\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector,VectorUDT\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Spark Context\n",
    "\n",
    "For these processes, we need to change the spark context parameters because we're using this notebook to dynamically insert data into Hive partitioned tables; this means we need to set the hive.exec.dynamic parameters. See the context configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([('spark.executor.memory', '8g'),\\\n",
    "                           ('spark.driver.memory', '6g'),\\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.sql.shuffle.partitions', 144), \\\n",
    "                           ('spark.default.parallelism', 144),\\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.executor.instances', 40), \\\n",
    "                           ('spark.local.dir','/mnt/FITs/Spark/tmp'),\\\n",
    "                           ('spark.executor.memoryOverhead', '6g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.rpc.message.maxSize', '512'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '1g'),\\\n",
    "                           ('spark.hadoop.hive.exec.dynamic.partition', True), \\\n",
    "                           ('spark.hadoop.hive.exec.dynamic.partition.mode','nonstrict'), \\\n",
    "                           ('spark.hadoop.hive.exec.max.dynamic.partitions', 100000), \\\n",
    "                           ('spark.hadoop.hive.exec.max.dynamic.partitions.pernode', 100000), \\\n",
    "                           ('spark.app.name','Fits on HDFS')])\n",
    "\n",
    "    # Stop the old context\n",
    "    #sc.stop()\n",
    "\n",
    "    # And restart the context with the new configuration\n",
    "    #sc = SparkContext(conf=conf)\n",
    "    #sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, clock\n",
    "class Timer:\n",
    "    \"\"\"\n",
    "    a simple class for printing time (s) since last call\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0=time()\n",
    "        self.t1=clock()\n",
    "        self.elapsed = 1\n",
    "        self.elapsedCPU = 1\n",
    "    \n",
    "    def start(self):\n",
    "        self.t0=time()\n",
    "        self.t1=clock()\n",
    "        \n",
    "    def stop(self):\n",
    "        t1=time()\n",
    "        t2=clock()\n",
    "        print(\"Elapsed {:2.1f}s, CPU {:2.1f}s\".format(t1-self.t0, t2-self.t1))\n",
    "        self.elapsed = t1-self.t0\n",
    "        self.elapsedCPU = t2-self.t1\n",
    "\n",
    "class DataframeError(Exception): \n",
    "  \n",
    "    # Constructor or Initializer \n",
    "    def __init__(self, value): \n",
    "        self.value = value \n",
    "  \n",
    "    # __str__ is to print() the value \n",
    "    def __str__(self): \n",
    "        return(repr(self.value)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The CreateAndSaveArrays function saves the positional data (in this case, Right Ascencion, Declination and Frequency) as an array element in a permanent Parquet table. the sparkfits_detail_arrays table is physically partitioned on the filename of the source FITS file and the array detail type (for example, 'RA -- SIN', 'DEC -- SIN ' and a spectra trype 'Hz', 'GHz' or 'km s-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAndSaveArrays(array, filename, arrayCoordinates, cubeSpectraUnits=None ):\n",
    "    if cubeSpectraUnits == None:\n",
    "        sda_detail_name=arrayCoordinates\n",
    "    else:\n",
    "        sda_detail_name=cubeSpectraUnits\n",
    "\n",
    "    #logger.in(filename, sda_detail_name )\n",
    "\n",
    "    arrayRDD=sc.parallelize(array).zipWithIndex().map(lambda x :(x[1], x[0]) )\n",
    "    arrayDF=arrayRDD.map(lambda x: (x[0], x[1].tolist()) ).toDF([\"index\", \"spectra\"])\n",
    "\n",
    "    #\"\"\"\n",
    "    #\n",
    "    # Maintaining order within a collect_list, See\n",
    "    # https://stackoverflow.com/questions/50766764/sorting-within-collect-list-in-hive\n",
    "    #\n",
    "    #\"\"\"\n",
    "\n",
    "    arrayDF.registerTempTable(\"SPECTRAS\")\n",
    "\n",
    "    sqlStmt=\"\"\"\n",
    "    with myData as\n",
    "    (\n",
    "        select 1 as grp,\n",
    "        index as idx,\n",
    "        map(\n",
    "            'spectraVal', spectra\n",
    "        ) as kv\n",
    "        from SPECTRAS\n",
    "        distribute by grp\n",
    "        sort by grp, index\n",
    "    )\n",
    "    select collect_list( float(a.kv['spectraVal']) ) as sda_detail_array \n",
    "    from myData a\n",
    "    group by grp\n",
    "    \"\"\"\n",
    "\n",
    "    spectraArrayDF=spark.sql(sqlStmt)\n",
    "    spectraArrayDF=spectraArrayDF.select(\"sda_detail_array\"\\\n",
    "                                         ,lit(filename).alias(\"sda_filename\")\\\n",
    "                                         ,lit(sda_detail_name).alias(\"sda_detail_type\"))\n",
    "\n",
    "    spectraArrayDF.registerTempTable(\"ARRAYS\")\n",
    "    spark.sql(\"\"\"\n",
    "    insert overwrite table fits_investigation.sparkfits_detail_arrays \n",
    "    partition(sda_filename , sda_detail_type) \n",
    "    select sda_detail_array, sda_filename, sda_detail_type\n",
    "    from ARRAYS\n",
    "    \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and save the source FITS file header data to a Parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFITSHeaderData(header, fitsFilename):\n",
    "    hduArray=np.empty((0, 4))\n",
    "    Results=Row(\"fits_file\",\"index\",\"key\",\"value\",\"comment\")\n",
    "    for i in np.arange(len(header)):\n",
    "        theKey=list(header.keys())[int(i)]\n",
    "        theValue=header[int(i)]\n",
    "        theComment=header.comments[int(i)]\n",
    "\n",
    "        #print(fitsFilename, theKey, theValue, theComment)\n",
    "        if theKey and not theKey.isspace():\n",
    "            result=Results(fitsFilename, int(i), theKey, str(theValue), header.comments[int(i)])\n",
    "\n",
    "            if i == 0:\n",
    "                hduDF=spark.createDataFrame([result])\n",
    "            else:\n",
    "                newRow=spark.createDataFrame([result])\n",
    "                hduDF = hduDF.union(newRow)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    hduDF.printSchema()\n",
    "    hduDF.registerTempTable(\"HEADERS\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "    insert overwrite table fits_investigation.sparkfits_fits_headers partition(sfh_fits_file ) \n",
    "    select index, key, value, comment, fits_file from HEADERS\n",
    "    \"\"\")\n",
    "\n",
    "    print('Header data created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowdf_into_imagerdd(df, final_num_partition=1):\n",
    "    \"\"\"\n",
    "    Reshape a DataFrame of rows into a RDD containing the full image\n",
    "    in one partition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame of image rows.\n",
    "    final_num_partition : Int\n",
    "        The final number of partitions. Must be one (default) unless you\n",
    "        know what you are doing.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    imageRDD : RDD\n",
    "        RDD containing the full image in one partition\n",
    "        \n",
    "    NOTE: With larger FITS files, we need to avoid the use of .glom() as it results in larger memort usage.\n",
    "        From the documentation -\n",
    "        .glom(self) - Return an RDD created by coalescing all elements within each partition into a list.\n",
    "        \n",
    "    return df.rdd.zipWithIndex().coalesce(final_num_partition).glom()    \n",
    "    \"\"\"\n",
    "\n",
    "    return df.rdd.zipWithIndex() #.coalesce(final_num_partition).glom()\n",
    "\n",
    "def clean_image(im):\n",
    "    \"\"\"\n",
    "    Cleans out the Nan elements in the array\n",
    "    \"\"\"\n",
    "    #im=im[~np.isnan(im).any(axis=1)]\n",
    "    im = np.nan_to_num(im)\n",
    "    return im\n",
    "\n",
    "\n",
    "def replace_nan_with_float(im):\n",
    "    \"\"\"\n",
    "    Replaces Nan elements with a random float\n",
    "    \"\"\"\n",
    "    \n",
    "    #nan_mask = np.isnan( np.array(im, dtype=np.float32) )\n",
    "    nan_mask = np.isnan( np.array(im) )\n",
    "    \n",
    "    sampl = np.random.uniform(low=0.01, high=13.3, size=(im.shape))\n",
    "    \n",
    "    im[nan_mask] = sampl[nan_mask]\n",
    "    \n",
    "    #random_replace = np.vectorize(lambda x: np.random.random() if np.isnan(x) else x)\n",
    "    #im=random_replace(im)\n",
    "    return im\n",
    "    \n",
    "\n",
    "def calculateBand(x, bandCount):\n",
    "    x = x // bandCount\n",
    "    return x\n",
    "\n",
    "def CastStructToArray(StructCol):\n",
    "    imageArray = np.asarray(StructCol[0]).tolist()\n",
    "    return imageArray\n",
    "\n",
    "def calculateBand(x, bandCount=32):\n",
    "    x = x  // bandCount\n",
    "    return x\n",
    "\n",
    "def dummyReturn(x):\n",
    "    return x.tolist()\n",
    "\n",
    "def returnAsIs(x):\n",
    "    return x\n",
    "\n",
    "# ====================================================\n",
    "# Write results to Parquet table\n",
    "# ====================================================\n",
    "\n",
    "def writeResults(sc, resultDF, vMode, vFormat, vTable):\n",
    "    \n",
    "    resultDF.write.mode(vMode).format(vFormat).saveAsTable(vTable)\n",
    "    \n",
    "def GetCubePositionData(cube, fileName, cubeDecCoordinate, cubeSpectraUnits, cubeDecDim):\n",
    "    \n",
    "    _, dec, _ = cube.world[0, :, 0]  #extract latitude world coordinates from cube dec\n",
    "    _, _, ra = cube.world[0, 0, :]  #extract longitude world coordinates from cube ra\n",
    "\n",
    "    \"\"\"\n",
    "    Make sure that we cast the Ra and Dec arrays explicitly to float32\n",
    "    \"\"\"\n",
    "    ra=np.array(ra, dtype=np.float32)\n",
    "    # and flip - always want declination ordered from greatest to lowest. eg -18 ..... -27 or 43 ..... 21\n",
    "    #if max(dec) <0 or 1==1:\n",
    "    \n",
    "    dec=np.flip( np.array(dec, dtype=np.float32) )\n",
    "    \n",
    "    #else:\n",
    "    #    dec=np.array(dec, dtype=np.float32)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Get the Spectral Axis data.\n",
    "    Note: Further enhancements can include changing the values of the Spectral axis - ie change from Hz to redshift or\n",
    "    velocity.\n",
    "    \n",
    "    \"\"\"\n",
    "    spectralAxis=cube.spectral_axis\n",
    "    \n",
    "    \"\"\"\n",
    "    Cast the SpectralCube explicitily to a numpy array, float 32\n",
    "    \"\"\"\n",
    "    \n",
    "    specArray=np.array(spectralAxis , dtype=np.float32)\n",
    "    \n",
    "    print(\"arrays created\")\n",
    "    \"\"\" parallelize the arrays \n",
    "    \n",
    "    NOTE: \n",
    "    As spark does not necessarily maintain the order of the values in a join, we need to specifically define the ordering\n",
    "    in order to maintain the correct relationships between the position and spectra arrays we extract from te FITS cube via \n",
    "    SpectralCube during the population process.\n",
    "    \n",
    "    To do this, we create a parallelised array for the spectra and declinatino with a zip index. the cartesian join is then sorted \n",
    "    spectra and declination indexes, and then the index is created for the cartesian RDD.\n",
    "    \n",
    "    This ensures the proper sorting and ordering of the combined data.\n",
    "    \n",
    "    Note that this is a potentially an expensive computational operation on larget arrays, but as it is only a one-off\n",
    "    computation per file, this is acceptable.\n",
    "    \"\"\"\n",
    "    rdd1=sc.parallelize(specArray).zipWithIndex()\n",
    "    rdd2=sc.parallelize(dec).zipWithIndex()\n",
    "    \n",
    "    print(\"and parallelized\")\n",
    "    \n",
    "    \"\"\" Ordered cartesian join as a basis of the dataframe \"\"\"\n",
    "    fDecDF=rdd1.cartesian(rdd2)\\\n",
    "    .sortBy(lambda x:(x[0][1], x[1][1]), True)\\\n",
    "    .zipWithIndex()\\\n",
    "    .map(lambda x: (x[0][0][0].tolist(), x[0][1][0].tolist(), x[1]))\\\n",
    "    .toDF([\"spd_spectra_value\",\"spd_declination\", \"spd_index\"])\\\n",
    "    .select(\"spd_index\", \\\n",
    "                castToFloat(\"spd_spectra_value\").alias(\"spd_spectra_value\"), \\\n",
    "                castToFloat(\"spd_declination\").alias(\"spd_declination\"), \\\n",
    "                lit(fileName).alias(\"spd_filename\"), \\\n",
    "                lit(cubeDecCoordinate).alias(\"spd_position_type\"), \\\n",
    "                lit(cubeSpectraUnits).alias(\"spd_spectra_type\"), \\\n",
    "                GetBandUDF(\"spd_index\", lit(cubeDecDim)).alias(\"spd_band\")  )                \n",
    "    print(\"dataframe created\")\n",
    "    return ra, dec, specArray, fDecDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetBandUDF = udf(lambda arr, bw: calculateBand(np.int(arr), bw), iType)\n",
    "CastToInt = udf(lambda x: dummyReturn(x), iType)\n",
    "CastToFloatArrayUDF = udf(lambda x : CastStructToArray(x), ArrayType(fType))\n",
    "\n",
    "castToFloat = udf(lambda arr: returnAsIs(arr), fType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use fits_investigation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----------+\n",
      "|          database|           tableName|isTemporary|\n",
      "+------------------+--------------------+-----------+\n",
      "|fits_investigation|          array_test|      false|\n",
      "|fits_investigation|    fits_header_data|      false|\n",
      "|fits_investigation|          float_test|      false|\n",
      "|fits_investigation|  imagetestsparkfits|      false|\n",
      "|fits_investigation|   localparalleltest|      false|\n",
      "|fits_investigation|localparalleltest...|      false|\n",
      "|fits_investigation|localparalleltest...|      false|\n",
      "|fits_investigation|localparalleltest...|      false|\n",
      "|fits_investigation|  newdfdeletemelater|      false|\n",
      "|fits_investigation|     parquetdataview|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetpartitione...|      false|\n",
      "|fits_investigation|parquetunpartitio...|      false|\n",
      "+------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "UseResidual=False\n",
    "\n",
    "if UseResidual:\n",
    "    bigFilePath='hdfs:///user/hduser/FITS_Files/residual.i.SB8170.cube.fits'\n",
    "    bigFileName=\"residual.i.SB8170.cube.fits\"\n",
    "    localBigFile=\"/mnt/FITs/residual.i.SB8170.cube.fits\"\n",
    "else:\n",
    "    bigFilePath='hdfs:///user/hduser/FITS_Files/image.restored.i.SB2338.V2.cube.fits'\n",
    "    bigFileName=\"image.restored.i.SB2338.V2.cube.fits\"\n",
    "    localBigFile=\"/mnt1/FITs/image.restored.i.SB2338.V2.cube.fits\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /mnt1/FITs/image.restored.i.SB2338.V2.cube.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU      77   (5607, 5654, 1, 2592)   float32   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could not memory map array with mode='readonly', falling back to mode='denywrite', which means that the array will be read-only [astropy.io.fits.file]\n",
      "WARNING:astropy:Could not memory map array with mode='readonly', falling back to mode='denywrite', which means that the array will be read-only\n",
      "WARNING: StokesWarning: Cube is a Stokes cube, returning spectral cube for I component [spectral_cube.spectral_cube]\n",
      "WARNING:astropy:StokesWarning: Cube is a Stokes cube, returning spectral cube for I component\n"
     ]
    }
   ],
   "source": [
    "timer=Timer()\n",
    "hdu=0\n",
    "df = sqlContext.read.format(\"fits\").option(\"hdu\", hdu).load(bigFilePath)\n",
    "fits.info(localBigFile)\n",
    "hdu1=fits.open(localBigFile , memmap=True)\n",
    "cubeHeader=hdu1[0].header\n",
    "cubeWCS=WCS(cubeHeader)\n",
    "cubeHz = SpectralCube.read(hdu1)\n",
    "\n",
    "hdu1.close()\n",
    "PartitionFileName = bigFileName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the header data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fits_file: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n",
      "Header data created\n"
     ]
    }
   ],
   "source": [
    "CreateFITSHeaderData(cubeHeader, PartitionFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image.restored.i.SB2338.V2.cube.fits'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PartitionFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarisation=cubeHeader[\"NAXIS3\"] # Polarisation 196608\n",
    "raLength=cubeHz.header[\"NAXIS1\"] # RA\n",
    "decLength=cubeHz.header[\"NAXIS2\"] # DEC\n",
    "spectraLength=cubeHz.header[\"NAXIS3\"] # Spectra (hx) Pixels \n",
    "\n",
    "numPolarisation=cubeHeader[\"NAXIS4\"]\n",
    "\n",
    "## Same as dec length, used in the GetBandUDF UDF functionsto calculate the polarisation and spectral band indexes.\n",
    "## See\n",
    "##        GetBandUDF(col(\"spi_index\"), lit(frequencyBandLength*spectraLength)).alias(\"spi_pol\"),\\\n",
    "##        GetBandUDF(col(\"spi_index\"), lit(frequencyBandLength)).alias(\"spi_band\") )\\\n",
    "\n",
    "\n",
    "frequencyBandLength=cubeHz.header[\"NAXIS2\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Sparkfits dataframe, and then populate the images table with the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file partition numbers - 2449\n",
      "RDD with zip index created\n",
      "Elapsed 542.2s, CPU 0.3s\n",
      "Dataframe created and persisted\n",
      "Elapsed 0.4s, CPU 0.0s\n"
     ]
    }
   ],
   "source": [
    "timer.start()\n",
    "partitionTarget=df.rdd.getNumPartitions()\n",
    "print(\"Source file partition numbers - {}\".format(partitionTarget))\n",
    "\n",
    "### Create the RDD with the zip index\n",
    "imageRDD=rowdf_into_imagerdd(df)\n",
    "\n",
    "print(\"RDD with zip index created\")\n",
    "timer.stop()\n",
    "\n",
    "timer.start()\n",
    "bRemoveNaNs=False\n",
    "\n",
    "if bRemoveNaNs:\n",
    "    print(\"Replacing NaN values with random float\")\n",
    "    PartitionFileName=PartitionFileName+'.NaNs.Removed'\n",
    "    imageDF=imageRDD.map(lambda x: ( replace_nan_with_float(np.array(x[0]) ).tolist(), x[1] ) ).toDF([\"spi_image\",\"spi_index\"])\n",
    "else:\n",
    "    imageDF=imageRDD.toDF([\"spi_image\",\"spi_index\"])\n",
    "\n",
    "\n",
    "numPartitions = imageDF.rdd.getNumPartitions()\n",
    "\n",
    "if numPartitions < partitionTarget:\n",
    "    imageDF=imageDF.repartition(partitionTarget)\n",
    "    print(\"Repartitioning the dataframe from {} to {} partitions...\".format(numPartitions, partitionTarget))\n",
    "\n",
    "dff=imageDF.select(\\\n",
    "                   \"spi_index\", \\\n",
    "                   CastToFloatArrayUDF(\"spi_image\").alias(\"spi_image\"), \\\n",
    "                   lit(PartitionFileName).alias(\"spi_filename\"), \\\n",
    "                   GetBandUDF(\"spi_index\", lit(frequencyBandLength*spectraLength)).alias(\"spi_pol\"),\\\n",
    "                   GetBandUDF(\"spi_index\", lit(frequencyBandLength)).alias(\"spi_band\") )\\\n",
    ".persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "print(\"Dataframe created and persisted\")\n",
    "\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spi_index: long (nullable = true)\n",
      " |-- spi_image: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- spi_filename: string (nullable = false)\n",
      " |-- spi_pol: integer (nullable = true)\n",
      " |-- spi_band: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.registerTempTable(\"IMAGES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------+\n",
      "|spi_index|           spi_image|        spi_filename|spi_band|\n",
      "+---------+--------------------+--------------------+--------+\n",
      "|        0|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|        1|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|        2|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|        3|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|        4|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|        5|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|        6|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|        7|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|        8|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|        9|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       10|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       11|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       12|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       13|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       14|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       15|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       16|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       17|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       18|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "|       19|[0.0, 0.0, 0.0, 0...|image.restored.i....|       0|\n",
      "+---------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select spi_index, spi_image, spi_filename, spi_band from IMAGES\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the dataframe into the target table.\n",
    "\n",
    "Check the polarisations. If there is only one polarisation, we can ignore that and insert into the sparkfits_images table; otherwise we insert into sparkfits_polarised_images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataframe to parquet images table\n",
      "Saving to sparkfits_images\n",
      "All done!\n",
      "Elapsed 1749.8s, CPU 0.5s\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing dataframe to parquet images table\")\n",
    "timer.start()\n",
    "if polarisation==1:\n",
    "    print(\"Saving to sparkfits_images\")\n",
    "    spark.sql(\"\"\"\n",
    "    insert overwrite table fits_investigation.sparkfits_images partition(spi_filename, spi_band ) \n",
    "    select spi_index, spi_image, spi_filename, spi_band from IMAGES\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"Saving to sparkfits_polarised_images\")\n",
    "    spark.sql(\"\"\"\n",
    "    insert overwrite table fits_investigation.sparkfits_polarised_images partition(spi_filename, spi_pol, spi_band ) \n",
    "    select spi_index, spi_image, spi_filename, spi_pol, spi_band from IMAGES\n",
    "    \"\"\")\n",
    "\n",
    "print(\"All done!\")\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Table Population\n",
    "\n",
    "The example below create dimension tables for cubes with differing spectral units. We create two new SpectralCube objects with velocity and GHz spectra,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the dimension tables\n",
      "Create the alternative spectral cubes for the differing spectral arrays\n",
      "Alternative cubes created\n",
      "Elapsed 0.1s, CPU 0.1s\n"
     ]
    }
   ],
   "source": [
    "print(\"Populating the dimension tables\")\n",
    "\n",
    "timer.start()\n",
    "print(\"Create the alternative spectral cubes for the differing spectral arrays\")\n",
    "cubeVel = cubeHz.with_spectral_unit(u.km / u.s, velocity_convention='radio' ,rest_value=200 * u.Hz)\n",
    "cubeGHz = cubeHz.with_spectral_unit(u.GHz, velocity_convention='radio' ,rest_value=200 * u.GHz)\n",
    "print(\"Alternative cubes created\")\n",
    "timer.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate([cubeHz, cubeVel, cubeGHz]):\n",
    "    timer.start()\n",
    "    print(c.header[\"ORIGIN\"])\n",
    "\n",
    "    cubePixelValue = c.header[\"BUNIT\"]\n",
    "    cubeRaDim = c.header[\"NAXIS1\"]\n",
    "    cubeDecDim = c.header[\"NAXIS2\"]\n",
    "    cubeSpectraDim = c.header[\"NAXIS3\"]\n",
    "    cubeRACoordinate = c.header[\"CTYPE1\"]\n",
    "    cubeDecCoordinate = c.header[\"CTYPE2\"]\n",
    "    cubeSpectraCoordinate = c.header[\"CTYPE3\"]\n",
    "    cubeRAUnits = c.header[\"CUNIT1\"]\n",
    "    cubeDecUnits = c.header[\"CUNIT2\"]\n",
    "    cubeSpectraUnits = c.header[\"CUNIT3\"]\n",
    "    print(cubePixelValue, cubeSpectraCoordinate, cubeSpectraUnits)\n",
    "    timer.start()\n",
    "\n",
    "    ra, dec, specArray, DecDF = GetCubePositionData(c, PartitionFileName, cubeDecCoordinate, cubeSpectraUnits, cubeDecDim)\n",
    "\n",
    "    \"\"\"\n",
    "    Add the declination and frequency dimension to the table\n",
    "    \"\"\"\n",
    "    print(\"Creating the temp table POSDIM \")\n",
    "\n",
    "    DecDF.registerTempTable(\"POSDIM\")\n",
    "    \n",
    "    newPosDimDF=spark.sql(\"\"\"\n",
    "        with myData as\n",
    "        (\n",
    "        select\n",
    "        spd_filename,spd_position_type,spd_spectra_type,\n",
    "        spd_declination,\n",
    "            map(\n",
    "                'idxVal', spd_index\n",
    "            ) as kv\n",
    "            from POSDIM\n",
    "            distribute by spd_declination\n",
    "            sort by spd_index\n",
    "        )\n",
    "        select\n",
    "        spd_filename,spd_position_type,spd_spectra_type,\n",
    "        spd_declination,\n",
    "        collect_list( float(a.kv['idxVal']) ) as spd_declination_spectra \n",
    "        from myData a\n",
    "        group by spd_filename,spd_position_type,spd_spectra_type,spd_declination\n",
    "        order by spd_declination desc\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    newPosDimDF.registerTempTable('NewPosDim')\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    insert overwrite table fits_investigation.sparkfits_new_position_dimension\n",
    "    partition(spd_filename, spd_position_type, spd_spectra_type) \n",
    "    select spd_declination,spd_declination_spectra,\n",
    "    spd_filename,spd_position_type,spd_spectra_type\n",
    "    from NewPosDim\n",
    "    \"\"\")\n",
    "    \n",
    "    if False:\n",
    "        spark.sql(\"\"\"\n",
    "        insert overwrite table fits_investigation.sparkfits_position_dimension \n",
    "        partition(spd_filename, spd_position_type, spd_spectra_type, spd_band) \n",
    "        select spd_index, spd_spectra_value, spd_declination,\n",
    "        spd_filename,spd_position_type,spd_spectra_type,spd_band\n",
    "        from POSDIM\n",
    "        \"\"\")\n",
    "\n",
    "    if i == 0:\n",
    "        print(\"Creating ra and dec detail arrays\")\n",
    "        CreateAndSaveArrays(ra,  PartitionFileName, cubeRACoordinate)\n",
    "        CreateAndSaveArrays(dec, PartitionFileName, cubeDecCoordinate)\n",
    "\n",
    "    print(\"Creating the spectral detail array\")\n",
    "    CreateAndSaveArrays(specArray, PartitionFileName, cubeSpectraCoordinate, cubeSpectraUnits) \n",
    "    timer.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas and SparkFits (Spark 2.3.0, python 3.6, increased resources)",
   "language": "python",
   "name": "sparkfits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
